{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be7cb2d",
   "metadata": {},
   "source": [
    "\n",
    "# TP2 — Redes Neuronales con Keras (Adult Income)\n",
    "\n",
    "> **Alumno/a:** _Completar_  \n",
    "> **Materia:** _Completar_  \n",
    "> **Fecha:** 2025-11-10\n",
    "\n",
    "Este notebook implementa una red neuronal con **Keras (TensorFlow)** para el dataset **Adult Income** (el mismo del TP1).  \n",
    "Incluye:\n",
    "- Preprocesamiento (one‑hot encoding + escalado de numéricas)\n",
    "- División en **train / validación / test**\n",
    "- **3 sets** distintos de hiperparámetros (profundidad, neuronas, dropout, learning rate)\n",
    "- Entrenamiento y evaluación con **accuracy** (y matriz de confusión opcional)\n",
    "- Selección del mejor set por **accuracy en validación**\n",
    "- Evaluación final en **test**\n",
    "- Espacio para **comparar** con lo obtenido en el TP1\n",
    "\n",
    "> Nota: Si no tenés el CSV local, el notebook intentará descargarlo automáticamente de UCI/OpenML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa58b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Setup ====\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38df09",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Carga de datos\n",
    "\n",
    "Usamos el dataset **Adult Income**. El código intenta:\n",
    "1. Cargar `adult.csv` o `adult.data` locales (si existen).\n",
    "2. Descargar desde UCI / OpenML si no están (requiere internet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_load_adult():\n",
    "    # 1) CSV/ARFF locales conocidos\n",
    "    local_candidates = [\n",
    "        \"adult.csv\",\n",
    "        \"adult.data\",\n",
    "        \"adult/adult.csv\",\n",
    "        \"adult/adult.data\"\n",
    "    ]\n",
    "    for p in local_candidates:\n",
    "        if os.path.exists(p):\n",
    "            if p.endswith(\".csv\"):\n",
    "                df = pd.read_csv(p)\n",
    "            else:\n",
    "                # adult.data separado por coma sin encabezados\n",
    "                cols = [\n",
    "                    \"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\n",
    "                    \"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\n",
    "                    \"hours-per-week\",\"native-country\",\"income\"\n",
    "                ]\n",
    "                df = pd.read_csv(p, header=None, names=cols, na_values=\"?\\s*\", skipinitialspace=True)\n",
    "            print(f\"Cargado local: {p}\")\n",
    "            return df\n",
    "\n",
    "    # 2) Intento UCI\n",
    "    try:\n",
    "        uci_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "        cols = [\n",
    "            \"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\n",
    "            \"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\n",
    "            \"hours-per-week\",\"native-country\",\"income\"\n",
    "        ]\n",
    "        df = pd.read_csv(uci_url, header=None, names=cols, na_values=\"?\\s*\", skipinitialspace=True)\n",
    "        print(\"Descargado desde UCI.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo descargar desde UCI:\", e)\n",
    "\n",
    "    # 3) Intento OpenML\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        adult = fetch_openml('adult', version=2, as_frame=True)\n",
    "        df = adult.frame\n",
    "        # Ajuste de nombre/etiqueta si viniera distinto\n",
    "        if 'class' in df.columns and 'income' not in df.columns:\n",
    "            df = df.rename(columns={'class': 'income'})\n",
    "        print(\"Descargado desde OpenML.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo descargar desde OpenML:\", e)\n",
    "\n",
    "    raise FileNotFoundError(\"No se encontró dataset local ni conexión para descargar. \"\n",
    "                            \"Colocá un 'adult.csv' junto al notebook y reejecutá.\")\n",
    "\n",
    "df = try_load_adult()\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75b707",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Limpieza y preparación\n",
    "\n",
    "- Eliminamos filas con `NaN` (provenientes de valores `'?'`).\n",
    "- Definimos **features** y **target** (`income`: `>50K` vs `<=50K`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uniformar nombre de columna target si fuese necesario\n",
    "if 'income' not in df.columns:\n",
    "    # intentar inferir\n",
    "    cand = [c for c in df.columns if c.lower() in ('income','class','target','label')]\n",
    "    assert len(cand) == 1, f\"No se encontró target unívoco, columnas: {df.columns}\"\n",
    "    df = df.rename(columns={cand[0]: 'income'})\n",
    "\n",
    "# Drop NAs\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Target binario (str -> 0/1)\n",
    "df['income'] = df['income'].astype(str).str.strip()\n",
    "y = (df['income'] == '>50K').astype(int).values  # 1 si >50K, 0 si <=50K\n",
    "X = df.drop(columns=['income'])\n",
    "\n",
    "# Definir columnas numéricas y categóricas\n",
    "numeric_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"Numéricas:\", numeric_cols)\n",
    "print(\"Categóricas:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89070f",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Split: train / validación / test\n",
    "\n",
    "- Primero separamos **train (60%)** y **temp (40%)**  \n",
    "- Luego dividimos **temp** en **validación (20%)** y **test (20%)** (del total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d099c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.40, random_state=SEED, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc2641",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Pipeline de preprocesamiento\n",
    "\n",
    "- `OneHotEncoder` para categóricas (sin `drop` para mantener toda la info).\n",
    "- `StandardScaler` para numéricas.\n",
    "- Armamos un `ColumnTransformer` y lo **ajustamos solo con train**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "])\n",
    "\n",
    "X_train_pp = preprocess.fit_transform(X_train)\n",
    "X_val_pp   = preprocess.transform(X_val)\n",
    "X_test_pp  = preprocess.transform(X_test)\n",
    "\n",
    "input_dim = X_train_pp.shape[1]\n",
    "input_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc002a06",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Definición de modelos (3 sets de hiperparámetros)\n",
    "\n",
    "Creamos una función `build_model(**h)` y tres diccionarios de hiperparámetros:\n",
    "- **Set A (chico):** 1 capa oculta (64), `dropout=0.0`, `lr=1e-3`\n",
    "- **Set B (medio):** 2 capas (128, 64), `dropout=0.2`, `lr=5e-4`\n",
    "- **Set C (grande):** 3 capas (256, 128, 64), `dropout=0.3`, `lr=3e-4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_dim, hidden_layers=(64,), dropout=0.0, lr=1e-3, l2_reg=0.0):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation='relu',\n",
    "                               kernel_regularizer=keras.regularizers.l2(l2_reg) if l2_reg>0 else None))\n",
    "        if dropout > 0:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "hyperparams_list = [\n",
    "    {\"name\": \"Set A (chico)\",  \"hidden_layers\": (64,),              \"dropout\": 0.0, \"lr\": 1e-3, \"l2_reg\": 0.0, \"epochs\": 20, \"batch_size\": 256},\n",
    "    {\"name\": \"Set B (medio)\",  \"hidden_layers\": (128, 64),          \"dropout\": 0.2, \"lr\": 5e-4, \"l2_reg\": 1e-5, \"epochs\": 25, \"batch_size\": 256},\n",
    "    {\"name\": \"Set C (grande)\", \"hidden_layers\": (256, 128, 64),     \"dropout\": 0.3, \"lr\": 3e-4, \"l2_reg\": 1e-5, \"epochs\": 30, \"batch_size\": 512},\n",
    "]\n",
    "hyperparams_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981f5e2",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Entrenamiento y evaluación en **validación**\n",
    "\n",
    "Entrenamos cada set con `EarlyStopping` monitorizando `val_loss`.  \n",
    "Guardamos accuracy en validación y seleccionamos el mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda34af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "histories = {}\n",
    "val_scores = []\n",
    "\n",
    "for h in hyperparams_list:\n",
    "    print(\"\\n==== Entrenando:\", h[\"name\"], \"====\")\n",
    "    model = build_model(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=h[\"hidden_layers\"],\n",
    "        dropout=h[\"dropout\"],\n",
    "        lr=h[\"lr\"],\n",
    "        l2_reg=h[\"l2_reg\"]\n",
    "    )\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train_pp, y_train,\n",
    "        validation_data=(X_val_pp, y_val),\n",
    "        epochs=h[\"epochs\"],\n",
    "        batch_size=h[\"batch_size\"],\n",
    "        verbose=1,\n",
    "        callbacks=cb\n",
    "    )\n",
    "    histories[h[\"name\"]] = history.history\n",
    "\n",
    "    # Eval en validación\n",
    "    val_pred = (model.predict(X_val_pp) >= 0.5).astype(int).ravel()\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_scores.append((h[\"name\"], val_acc, model))\n",
    "    print(f\"Accuracy de validación ({h['name']}): {val_acc:.4f}\")\n",
    "\n",
    "# Ordenar por accuracy desc\n",
    "val_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "best_name, best_val_acc, best_model = val_scores[0]\n",
    "print(\"\\nMejor set por validación:\", best_name, \"— acc:\", round(best_val_acc, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1794d0",
   "metadata": {},
   "source": [
    "\n",
    "### Curvas de entrenamiento (loss y accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ebfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Una figura por métrica, sin estilos de color específicos\n",
    "plt.figure()\n",
    "for name, hist in histories.items():\n",
    "    plt.plot(hist['loss'], label=f\"{name} - train\")\n",
    "    plt.plot(hist['val_loss'], label=f\"{name} - val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Pérdida (train vs val)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for name, hist in histories.items():\n",
    "    plt.plot(hist['accuracy'], label=f\"{name} - train\")\n",
    "    plt.plot(hist['val_accuracy'], label=f\"{name} - val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy (train vs val)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044c959",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Evaluación en **test** (mejor set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pred = (best_model.predict(X_test_pp) >= 0.5).astype(int).ravel()\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(f\"Accuracy en test con '{best_name}': {test_acc:.4f}\")\n",
    "\n",
    "# Matriz de confusión y reporte (opcional)\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "print(\"\\nMatriz de confusión:\\n\", cm)\n",
    "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, test_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c129c8",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Comparación con TP1\n",
    "\n",
    "**Completar con los resultados de TP1** (por ejemplo):\n",
    "- Regresión logística: _accuracy = ..._\n",
    "- Árbol de decisión: _accuracy = ..._\n",
    "- (Otros si usaste)\n",
    "\n",
    "**Resumen comparativo:** _Escribir análisis: ¿mejoró Keras? ¿sobreajuste? ¿tiempos? ¿sensibilidad a hiperparámetros?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49037ce2",
   "metadata": {},
   "source": [
    "\n",
    "## (Opcional) Guardado de artefactos\n",
    "\n",
    "Guardamos el **preprocesador** (`sklearn`) y el **modelo** (`Keras`) para uso futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "outdir = pathlib.Path(\"artefactos_modelo\")\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "joblib.dump(preprocess, outdir / \"preprocess.joblib\")\n",
    "best_model.save(outdir / \"keras_best_model.keras\")\n",
    "print(\"Guardado en:\", outdir.resolve())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
